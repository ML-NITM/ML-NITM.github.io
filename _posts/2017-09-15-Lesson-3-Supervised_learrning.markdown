---
layout: post
title:  "Lesson-4: Supervised Learning -- Classification"
date:   2017-09-16 1:31:56 +0530
categories: lesson
---

# Supervised Learning -- Classification

To visualize the workings of machine learning algorithms, it is often helpful to study two-dimensional or one-dimensional data, that is data with only one or two features. While in practice, datasets usually have many more features, it is hard to plot high-dimensional data in on two-dimensional screens.

We will illustrate some very simple examples before we move on to more "real world" data sets.


```python
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
```


First, we will look at a two class classification problem in two dimensions. We use the synthetic data generated by the ``make_blobs`` function.


```python
from sklearn.datasets import make_blobs

X, y = make_blobs(centers=2, random_state=0)

print('X ~ n_samples x n_features:', X.shape)
print('y ~ n_samples:', y.shape)

print('\nFirst 5 samples:\n', X[:5, :])
print('\nFirst 5 labels:', y[:5])
```

    X ~ n_samples x n_features: (100, 2)
    y ~ n_samples: (100,)
    
    First 5 samples:
     [[ 4.21850347  2.23419161]
     [ 0.90779887  0.45984362]
     [-0.27652528  5.08127768]
     [ 0.08848433  2.32299086]
     [ 3.24329731  1.21460627]]
    
    First 5 labels: [1 1 0 0 1]


As the data is two-dimensional, we can plot each sample as a point in a two-dimensional coordinate system, with the first feature being the x-axis and the second feature being the y-axis.


```python
plt.scatter(X[y == 0, 0], X[y == 0, 1], 
            c='blue', s=40, label='0')
plt.scatter(X[y == 1, 0], X[y == 1, 1], 
            c='red', s=40, label='1')

plt.xlabel('first feature')
plt.ylabel('second feature')
plt.legend(loc='upper right');
```


![png](/images/05.Supervised_Learning-Classification-Bernard_files/05.Supervised_Learning-Classification-Bernard_6_0.png)


Classification is a supervised task, and since we are interested in its performance on unseen data, we split our data into two parts:

1. a training set that the learning algorithm uses to fit the model
2. a test set to evaluate the generalization performance of the model

The ``train_test_split`` function from the ``model_selection`` module does that for us -- we will use it to split a dataset into 75% training data and 25% test data.

<img src="/images/train_test_split_matrix.svg" width="100%">



```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.25,
                                                    random_state=1234,
                                                    stratify=y)
```

### The scikit-learn estimator API
<img src="/images/supervised_workflow.svg" width="100%">


Every algorithm is exposed in scikit-learn via an ''Estimator'' object. (All models in scikit-learn have a very consistent interface). For instance, we first import the logistic regression class.


```python
from sklearn.linear_model import LogisticRegression
```

Next, we instantiate the estimator object.


```python
classifier = LogisticRegression()
```


```python
X_train.shape
```




    (75, 2)




```python
y_train.shape
```




    (75,)



To built the model from our data, that is to learn how to classify new points, we call the ``fit`` function with the training data, and the corresponding training labels (the desired output for the training data point):


```python
classifier.fit(X_train, y_train)
```




    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
              verbose=0, warm_start=False)



(Some estimator methods such as `fit` return `self` by default. Thus, after executing the code snippet above, you will see the default parameters of this particular instance of `LogisticRegression`. Another way of retrieving the estimator's ininitialization parameters is to execute `classifier.get_params()`, which returns a parameter dictionary.)

We can then apply the model to unseen data and use the model to predict the estimated outcome using the ``predict`` method:


```python
prediction = classifier.predict(X_test)
```

We can compare these against the true labels:


```python
print(prediction)
print(y_test)
```

    [1 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0]
    [1 1 1 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 1 0]


We can evaluate our classifier quantitatively by measuring what fraction of predictions is correct. This is called **accuracy**:


```python
np.mean(prediction == y_test)
```




    0.83999999999999997



There is also a convenience function , ``score``, that all scikit-learn classifiers have to compute this directly from the test data:
    


```python
classifier.score(X_test, y_test)
```




    0.83999999999999997



It is often helpful to compare the generalization performance (on the test set) to the performance on the training set:


```python
classifier.score(X_train, y_train)
```




    0.94666666666666666



LogisticRegression is a so-called linear model,
that means it will create a decision that is linear in the input space. In 2d, this simply means it finds a line to separate the blue from the red:


```python
from figures import plot_2d_separator

plt.scatter(X[y == 0, 0], X[y == 0, 1], 
            c='blue', s=40, label='0')
plt.scatter(X[y == 1, 0], X[y == 1, 1], 
            c='red', s=40, label='1', marker='s')

plt.xlabel("first feature")
plt.ylabel("second feature")
plot_2d_separator(classifier, X)
plt.legend(loc='upper right');
```


![png](/images/05.Supervised_Learning-Classification-Bernard_files/05.Supervised_Learning-Classification-Bernard_30_0.png)


**Estimated parameters**: All the estimated model parameters are attributes of the estimator object ending by an underscore. Here, these are the coefficients and the offset of the line:


```python
print(classifier.coef_)
print(classifier.intercept_)
```

    [[ 1.38092515 -1.49993172]]
    [ 1.54995538]


Another classifier: K Nearest Neighbors
------------------------------------------------
Another popular and easy to understand classifier is K nearest neighbors (kNN).  It has one of the simplest learning strategies: given a new, unknown observation, look up in your reference database which ones have the closest features and assign the predominant class.

The interface is exactly the same as for ``LogisticRegression above``.


```python
from sklearn.neighbors import KNeighborsClassifier
```

This time we set a parameter of the KNeighborsClassifier to tell it we only want to look at one nearest neighbor:


```python
knn = KNeighborsClassifier(n_neighbors=1)
```

We fit the model with out training data


```python
knn.fit(X_train, y_train)
```




    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
               metric_params=None, n_jobs=1, n_neighbors=1, p=2,
               weights='uniform')




```python
plt.scatter(X[y == 0, 0], X[y == 0, 1], 
            c='blue', s=40, label='0')
plt.scatter(X[y == 1, 0], X[y == 1, 1], 
            c='red', s=40, label='1', marker='s')

plt.xlabel("first feature")
plt.ylabel("second feature")
plot_2d_separator(knn, X)
plt.legend(loc='upper right');
```


![png](/images/05.Supervised_Learning-Classification-Bernard_files/05.Supervised_Learning-Classification-Bernard_39_0.png)



```python
knn.score(X_test, y_test)
```




    1.0



<div class="alert alert-success">
    <b>EXERCISE</b>:
     <ul>
      <li>
      Apply the KNeighborsClassifier to the ``iris`` dataset. Play with different values of the ``n_neighbors`` and observe how training and test score change.
      </li>
    </ul>
</div>


```python
# %load solutions/05A_knn_with_diff_k.py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.25,
                                                    random_state=1234,
                                                    stratify=y)

X_trainsub, X_valid, y_trainsub, y_valid = train_test_split(X_train, y_train,
                                                            test_size=0.5,
                                                            random_state=1234,
                                                            stratify=y_train)
scores=[]
k_values=np.arange(1,10)

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    scores.append(knn.score(X_test,y_test))
    
plt.plot(k_values,scores)
plt.xlabel("NB Neighbors")
plt.ylabel("Accuracy")

    
```




    <matplotlib.text.Text at 0x7fb21cb2f2e8>




![png](/images/05.Supervised_Learning-Classification-Bernard_files/05.Supervised_Learning-Classification-Bernard_42_1.png)


 if you change the random_state to some other number the graph will change.

 References:

The content in this post is largely taken from the github repository [https://github.com/amueller/scipy-2017-sklearn](https://github.com/amueller/scipy-2017-sklearn)
